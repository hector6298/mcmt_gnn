# Configuration parameters for training (loss, optimizer, lr, datasets)
training_config:
  checkpoint_path_prefix: 'results'
  load_previous_train_state: true
  epochs: 1
  dataset:
    sequence_path_prefix: 'datasets/AIC20/'
    sequences_train: 
      - 'S01'
      - 'S04'
      - 'S03'
    sequences_val: 
      - 'S02'
    annotations_filename: 'gt.txt'
    num_ids_per_graph: 16
    embeddings_per_iteration: 48
    resized_img_shape: [224, 224]
    original_img_shape: [1920, 1080]
    train_batch_size: 1
    test_batch_size: 1
  loss:
    loss_type: 'ce_custom' # 'ce' - 'focal' - 'bce'
    alpha: [0.25, 0.75]
    gamma: 2
  lr_scheduler:
    scheduler_type: 'step'
    step_size: 50
    gamma: 0.1
    warmup_duration: 5
  optimizer:
    lr: 0.01
    momentum: 0.9
    dampening: 0
  reid:
    reid_model_path: 'models/reid/resnet101_ibn_a_2.pth'

# Configuration parameters for the GNN architecture
gnn_arch:
  node_agg_fn: 'sum'
  num_enc_steps: 1  # before 4 Number of message passing steps
  num_class_steps: 1  # before 3Number of message passing steps during feature vectors are classified (after Message Passing)
  reattach_initial_nodes: False  # elg. Original: False Determines whether initially encoded node feats are used during node updates
  reattach_initial_edges: False  #  elg. Original: TrueDetermines whether initially encoded edge feats are used during node updates

  encoder_feats_dict:
    edges:
      edge_in_dim: 2 
      edge_fc_dims: [4] 
      edge_out_dim: 4 

    nodes:
      node_in_dim: 2048
      node_fc_dims: [1024,512, 128]
      node_out_dim: 32
      dropout_p: 0.1
      use_batchnorm: True

  # MPN edge update
  edge_model_feats_dict:
    fc_dims: [4, 4] 
    dropout_p: 0.1
    use_batchnorm: True

  # MPN node update
  node_model_feats_dict:
    fc_dims: [32]  
    dropout_p: 0.1
    use_batchnorm: True

  # Classifier
  classifier_feats_dict:
    edge_in_dim: 4 
    edge_fc_dims: [] 
    edge_out_dim: 2
    dropout_p: 0
    use_batchnorm: False
    is_classifier: True